// Auto-generated
export const posts = [
  {
    "slug": "llm-from-scratch-tokenization",
    "title": "Build LLM from scratch Part 1 (Tokenization)",
    "tags": [
      "LLM",
      "GenAI",
      "Tokenizer"
    ],
    "category": "llm-from-scratch",
    "created": "2025-09-10T00:00:00.000Z",
    "updated": "2025-09-10T00:00:00.000Z",
    "raw": "# Theory behind Tokenization\r\n\r\nBefore a neural network can understand text, it needs to convert words into numbers. This is done through tokenization.\r\n## Why do we tokenize?\r\n - Computers can’t understand words directly — they need numerical representations.\r\n - Tokenization splits text into units (tokens) which are then mapped to IDs.\r\n\r\nThere are different approaches to tokenize text into numbers below are few approaches we will understand along with some pro's and con's in them:\r\n\r\n## Different tokenization approaches:\r\n 1. **Word-level tokenization** → \r\n    In the approach we will break text into word for an example `The cat sat` → `[The, cat, sat]`.\r\n    Simple python example for the same:\r\n\r\n    ```python\r\n    # Example: Word-level tokenization in Python\r\n    import re\r\n    # Sample text\r\n    text = \"Hello! I'm learning how to do word-level tokenization in Python.\"\r\n    # Method 1: Simple split (not very robust)\r\n    tokens_simple = text.split()\r\n    print(\"Simple split:\", tokens_simple)\r\n    ```\r\n    Output:\r\n    ```bash\r\n    Simple split: ['Hello!', \"I'm\", 'learning', 'how', 'to', 'do', 'word-level', 'tokenization', 'in', 'Python.']\r\n    ```\r\n    But what if some new word comes in future this struggles. Like in 11th addition of oxford dictionary we have around 600,000 words just for English imagine how many words there in multiple other languges its Huge right so it is practically not possible to tokenize all\r\n\r\n 2. **Character-level tokenization** → \r\n    - Every character is a token (`\"cat\" → [c, a, t]`). Very flexible, but inefficient (longer sequences).\r\n    - Tokens wont provide any meaningful word so in practical we need longer sequences to form a word it self so even longer sequence to form meaning ful sentence.\r\n 3. **Subword tokenization (BPE/Byte-Level BPE)** →\r\n    - Breaks words into frequent subwords.\r\n    - Ex: \"unhappiness\" → [un, happi, ness].\r\n    - Balances efficiency and flexibility.\r\n    - GPT-2 and some of newer models uses Byte-Level BPE:\r\n        - Operates at raw byte level.\r\n        - Supports all Unicode characters.\r\n        - No out-of-vocab issues.\r\n\r\n## Why BPE (Byte Pair Encoding)?\r\n- Handles rare words by splitting them into smaller pieces.\r\n- Keeps frequent words as single tokens.\r\n- Works at the byte-level → supports all languages and characters.\r\n\r\n**We will add some special charactors also as tokens while training our BPE tokenizer:**\r\n- `<pad>` → for padding sequences.\r\n- `<unk>` → for unknown tokens (rarely used in BPE).\r\n- `<|endoftext|>` → signals the end of input.\r\n- `<mask>` → used for masked training (BERT-style, not GPT).\r\n\r\n**BPE tokenizer** is comparitively complex to implement then **Word-level tokenization** or **Character-level tokenization** but we don't have to worry we have readymade python libraries available which are effeciant\r\n\r\n# Practical\r\n\r\nBefore you execute the below code run `pip install tokenizers`. This installs `tokenizers` library into your python virtual environment\r\n\r\n```python\r\n\r\nimport os\r\nfrom tokenizers import ByteLevelBPETokenizer  # Add this import\r\n\r\ndef train_tokenizer_from_texts(texts, vocab_size=30000, save_dir=\"tokenizer-dir\"):\r\n    \"\"\"\r\n    Train a new ByteLevelBPE tokenizer from scratch on your dataset.\r\n    \"\"\"\r\n    os.makedirs(save_dir, exist_ok=True)\r\n\r\n    # Write texts to a temporary file (needed by tokenizers lib)\r\n    temp_file = os.path.join(save_dir, \"train_texts.txt\")\r\n    with open(temp_file, \"w\", encoding=\"utf-8\") as f:\r\n        for line in texts:\r\n            f.write(line.strip() + \"\\n\")\r\n\r\n    tokenizer = ByteLevelBPETokenizer()\r\n    tokenizer.train(files=[temp_file], vocab_size=vocab_size, min_frequency=2,\r\n                    special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\", \"<|endoftext|>\"])\r\n\r\n    tokenizer.save_model(save_dir)\r\n    print(f\"✅ Trained new tokenizer with vocab size {vocab_size}, saved to {save_dir}\")\r\n\r\n    vocab_file = os.path.join(save_dir, \"vocab.json\")\r\n    merges_file = os.path.join(save_dir, \"merges.txt\")\r\n    return ByteLevelBPETokenizer(vocab_file, merges_file)\r\n\r\ndef ensure_gpt2_bpe_files(texts,vocabsize, vocab_file=\"tokenizer-dir/vocab.json\", merges_file=\"tokenizer-dir/merges.txt\"):\r\n    \"\"\"\r\n    Create tokenizer if not present.\r\n    \"\"\"\r\n\r\n    if not os.path.exists(vocab_file) or not os.path.exists(merges_file):\r\n        train_tokenizer_from_texts(texts, vocab_size=vocabsize, save_dir=\"tokenizer-dir\")\r\n\r\n\r\ndef get_custom_tokenizer(texts, vocabsize = 30000, vocab_file=\"tokenizer-dir/vocab.json\", merges_file=\"tokenizer-dir/merges.txt\"):\r\n    \"\"\"\r\n    Load a ByteLevelBPETokenizer from vocab and merges files.\r\n    \"\"\"\r\n    ensure_gpt2_bpe_files(texts,vocabsize, vocab_file, merges_file)\r\n    tokenizer = ByteLevelBPETokenizer(vocab_file, merges_file)\r\n    return tokenizer\r\n\r\ntokenizer = get_custom_tokenizer([\"Hello world here is tokenizer please welcome\"],15)\r\ndecoded = tokenizer.encode(\"Hello world\").ids\r\nprint(decoded)\r\nencoded = tokenizer.decode(decoded)\r\nprint(encoded)\r\n\r\n## new word not in trianing data\r\nprint(\"================== new word =================\")\r\ndecoded = tokenizer.encode(\"Cat sat on a wall\").ids\r\nprint(decoded)\r\nencoded = tokenizer.decode(decoded)\r\nprint(encoded)\r\n```\r\nWhen you run the above code you see following output\r\n\r\n```bash\r\n[45, 74, 81, 81, 84, 226, 92, 84, 87, 81, 73]\r\nHello world\r\n================== new word =================\r\n[40, 70, 89, 226, 88, 70, 89, 226, 84, 83, 226, 70, 226, 92, 70, 81, 81]\r\nCat sat on a wall\r\n```\r\n\r\n## Lets dig little bit in above code\r\n- **Training stage**: First we have to train the tokenizer with training dataset to make to simple i trained it with a static texts. This generates 2 files `vocab.json` and `merges.txt` which containers generated vocabilary and merges of vocab respectively\r\n- **encode decode stage**: After training your tokenizer is ready to do encode given text into tokens and decode tokens into text back. One thing you can observe is it can handle new words it never seen during training also prety well this is because of byte level division of words helps it to interpret better\r\n- **Thumb rule**: Whenever you create a tokenizer you should make sure a text encoded then decoded should exactly same a original text passed. Because tokenizer is just numerical representation of data it should manipulate or modify original\r\n\r\n\r\n",
    "excerpt": "# Theory behind Tokenization Before a neural network can und…"
  }
];
export const about = [
  {
    "slug": "about",
    "title": "About Me",
    "tags": [
      "about"
    ],
    "category": "pages",
    "created": "2025-08-26T00:00:00.000Z",
    "updated": "2025-08-26T00:00:00.000Z",
    "raw": "\r\nHello! I'm **Hafeez Shaik**. I have over 7 years experience in Software Engineering.\r\nMy experiences includes GenAI, Prompt Engineering, AI Agents, AWS Cloud, Chat bots, Python Fast API applications, Java Spring boot applications, ReactTS\r\n\r\n# Summary\r\n- Python, Prompt Engineering, GenAI, Java, AI Agents, Fast API, AWS, Microservice Developer with 7+ years experience\r\n- Skilled in Python, Java, Spring boot (REST Web services, JSON), Spring, PCF, Cloud, AWS certified, IBM MQ, Unit and integration test with Junit and Mockito, Jenkins, Relational db(MySql, Mariadb, Oracle), NoSql(Redis, OpenSearch, MangoDB).\r\n- Strong engineering professional with knowledge on secured coding and OWASP vulnerabilities.\r\n- Having Experience in Agile process with use of issue tracking systems like Jira.\r\n- Bachelor's degree focused in Computer Science from KL University. Demonstrate the ability to work independently and manage multiple tasks.\r\n\r\n# Experience\r\nI am currently working in JP Morgan & Chase as Software Developer\r\nLearn more about my exprience in [Linkedin](https://www.linkedin.com/in/shaik--hafeez/)\r\n\r\n# Education\r\n- **Bachelor of Technology (BTech)** 2014-2018\r\nComputer Science and Engineering(CSE) from KL University, Vijayawada, India\r\n\r\n# Certificates\r\n- **AWS developer associate(DVA CO2)** - 2024 [AWS](https://www.credly.com/badges/3e7f7652-4ad2-4472-ae78-bd731021d8c4/public_url)\r\n- **Problem Solving** - 2020 [Hacker Rank](https://www.hackerrank.com/certificates/c74aa8ff0d74)\r\n- **AWS developer associate(DVA CO1)** - 2020 [AWS](https://www.credly.com/badges/7c5047b4-ac67-4ea6-a65c-7e1ec415d880?source=linked_in_profile)\r\n- **Silver application Security Titan** - 2020 [DBS Bank](https://drive.google.com/file/d/1cwKbORvoMHqwys0cdBWnzmyWGa0mkvFX/view)\r\n- **Bronze application Security Titan** - 2019 [DBS Bank](https://drive.google.com/file/d/1JUL9BEc_LwwX-EMdkhyHg9piSiEkVfRn/view)\r\n- **Android App development** - 2016 [HPES India]()\r\n\r\n# Awards\r\nI did received multiple awards in different companies at different experiences level along and Won multiple Hackathons and technical events more you can find in my [LinkedIn](https://www.linkedin.com/in/shaik--hafeez/)",
    "excerpt": "Hello! I'm **Hafeez Shaik**. I have over 7 years experience …"
  }
];
  